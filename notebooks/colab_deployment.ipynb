{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Real-Time Semantic Segmentation - Google Colab Backend Deployment\n\nThis notebook deploys the **backend server** on Google Colab with GPU acceleration.\n\n## Split Architecture\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  YOUR LOCAL         ‚îÇ         ‚îÇ  GOOGLE COLAB        ‚îÇ\n‚îÇ  MACHINE            ‚îÇ         ‚îÇ  (Free GPU)          ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                     ‚îÇ         ‚îÇ                      ‚îÇ\n‚îÇ  Frontend Server    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  Backend Server      ‚îÇ\n‚îÇ  (Port 8080)        ‚îÇ  ngrok  ‚îÇ  (Port 8000)         ‚îÇ\n‚îÇ                     ‚îÇWebSocket‚îÇ                      ‚îÇ\n‚îÇ  - HTML/CSS/JS      ‚îÇ         ‚îÇ  - FastAPI           ‚îÇ\n‚îÇ  - Webcam capture   ‚îÇ         ‚îÇ  - PyTorch Models    ‚îÇ\n‚îÇ  - UI controls      ‚îÇ         ‚îÇ  - GPU Inference     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## Setup Instructions\n\n1. **Enable GPU**: Go to `Runtime` ‚Üí `Change runtime type` ‚Üí Select `GPU`\n2. **Get ngrok token**: Sign up at https://ngrok.com and get your auth token\n3. **Run cells 1-6**: Execute each cell sequentially\n4. **Copy ngrok URL**: From Cell 5 output (you'll paste this in your local frontend)\n5. **Start local frontend**: On your machine, run `./scripts/start_frontend.sh`\n6. **Connect**: Open http://localhost:8080 and paste the ngrok URL\n\n## Note\n- Colab sessions timeout after 12 hours or 90 minutes idle\n- GPU allocation is not guaranteed (T4/P100/V100 depending on availability)\n- Frontend runs locally for better performance and easier development"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repository (replace with your repo URL)\n",
    "# IMPORTANT: Make sure you've pushed the latest fixes to GitHub!\n",
    "\n",
    "# Remove any old version first\n",
    "!rm -rf /content/RealTimeSeg\n",
    "\n",
    "# Clone fresh version\n",
    "!git clone https://github.com/arti1117/RealTimeSeg.git\n",
    "%cd RealTimeSeg\n",
    "\n",
    "# Show last commit to verify you have latest version\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã Repository Status:\")\n",
    "print(\"=\"*70)\n",
    "!git log -1 --oneline\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"üîç Verifying Critical Bug Fixes...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_checks_passed = True\n",
    "\n",
    "# Check 1: Model initialization fix\n",
    "print(\"\\n‚úì Check 1: Model Initialization Fix\")\n",
    "inference_path = '/content/RealTimeSeg/backend/models/inference_engine.py'\n",
    "if os.path.exists(inference_path):\n",
    "    with open(inference_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        if 'if self.current_model is None or mode != self.current_mode:' in content:\n",
    "            print(\"  ‚úÖ PASS - Model initialization bug is fixed\")\n",
    "        else:\n",
    "            print(\"  ‚ùå FAIL - Model initialization bug NOT fixed!\")\n",
    "            print(\"  ‚Üí You need to push the latest changes to GitHub and re-clone\")\n",
    "            all_checks_passed = False\n",
    "else:\n",
    "    print(\"  ‚ùå FAIL - inference_engine.py not found\")\n",
    "    all_checks_passed = False\n",
    "\n",
    "# Check 2: Import fixes\n",
    "print(\"\\n‚úì Check 2: Absolute Imports Fix\")\n",
    "app_path = '/content/RealTimeSeg/backend/app.py'\n",
    "if os.path.exists(app_path):\n",
    "    with open(app_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        if 'from models import ModelLoader' in content or 'from models.model_loader import ModelLoader' in content:\n",
    "            print(\"  ‚úÖ PASS - Absolute imports are configured\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  WARNING - Check imports (should be absolute, not relative)\")\n",
    "else:\n",
    "    print(\"  ‚ùå FAIL - app.py not found\")\n",
    "    all_checks_passed = False\n",
    "\n",
    "# Check 3: Frontend canvas separation\n",
    "print(\"\\n‚úì Check 3: Frontend Canvas Separation Fix\")\n",
    "frontend_path = '/content/RealTimeSeg/frontend/index.html'\n",
    "if os.path.exists(frontend_path):\n",
    "    with open(frontend_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        if 'capture-canvas' in content and 'display-canvas' in content:\n",
    "            print(\"  ‚úÖ PASS - Separate canvases for capture and display\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  WARNING - Frontend might not have canvas separation fix\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  INFO - Frontend is served locally, not needed in Colab\")\n",
    "\n",
    "# Check 4: Initialize function exists\n",
    "print(\"\\n‚úì Check 4: Server Initialization Function\")\n",
    "if os.path.exists(app_path):\n",
    "    with open(app_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        if 'def initialize_server():' in content:\n",
    "            print(\"  ‚úÖ PASS - Explicit initialization function exists\")\n",
    "        else:\n",
    "            print(\"  ‚ùå FAIL - initialize_server() function not found\")\n",
    "            all_checks_passed = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if all_checks_passed:\n",
    "    print(\"‚úÖ ALL CRITICAL FIXES VERIFIED!\")\n",
    "    print(\"‚úÖ You can proceed to Cell 3 (Install Dependencies)\")\n",
    "else:\n",
    "    print(\"‚ùå SOME FIXES ARE MISSING!\")\n",
    "    print(\"‚ùå ACTION REQUIRED:\")\n",
    "    print(\"   1. On your local machine, run: git push origin main\")\n",
    "    print(\"   2. Come back to Colab and re-run Cell 2 (Clone Repository)\")\n",
    "    print(\"   3. Then re-run this cell to verify again\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Verify Code Fixes (Important!)\n",
    "\n",
    "This cell verifies that you have all the critical bug fixes. **All checks should pass!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support\n",
    "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install other requirements\n",
    "!pip install -r backend/requirements.txt\n",
    "\n",
    "# Install ngrok for tunneling\n",
    "!pip install pyngrok\n",
    "\n",
    "# Install nest_asyncio for Colab event loop compatibility\n",
    "!pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download and Cache Models (Optional)\n",
    "\n",
    "Pre-download models to speed up startup. Models will be cached for the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models.segmentation as models\n",
    "from transformers import SegformerForSemanticSegmentation, Mask2FormerForUniversalSegmentation\n",
    "\n",
    "print(\"Downloading models...\")\n",
    "\n",
    "# Download DeepLabV3 models\n",
    "print(\"1/4 Downloading DeepLabV3-MobileNetV3...\")\n",
    "_ = models.deeplabv3_mobilenet_v3_large(pretrained=True)\n",
    "\n",
    "print(\"2/4 Downloading DeepLabV3-ResNet50...\")\n",
    "_ = models.deeplabv3_resnet50(pretrained=True)\n",
    "\n",
    "print(\"3/4 Downloading SegFormer-B3...\")\n",
    "_ = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/segformer-b3-finetuned-ade-512-512\"\n",
    ")\n",
    "\n",
    "print(\"4/4 Downloading Mask2Former (SOTA) - this may take a while...\")\n",
    "_ = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
    "    \"facebook/mask2former-swin-large-ade-semantic\"\n",
    ")\n",
    "\n",
    "print(\"‚úì All models downloaded and cached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup ngrok Tunnel\n",
    "\n",
    "**Important**: \n",
    "1. Replace `YOUR_NGROK_TOKEN` with your actual token from https://ngrok.com\n",
    "2. **COPY THE URL** shown below - you'll paste it in your local frontend!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pyngrok import ngrok, conf\nimport os\n\n# Set your ngrok auth token here\nNGROK_TOKEN = \"\"  # Replace with your token from https://ngrok.com\n\n# Configure ngrok\nconf.get_default().auth_token = NGROK_TOKEN\n\n# Create tunnel\npublic_url = ngrok.connect(8000)\nprint(f\"\\n{'='*70}\")\nprint(f\"üåê PUBLIC URL: {public_url}\")\nprint(f\"{'='*70}\\n\")\nprint(f\"üìã COPY THIS URL - You'll paste it in your local frontend!\")\nprint(f\"\\nNext steps:\")\nprint(f\"1. Copy the URL above\")\nprint(f\"2. On your local machine, run: ./scripts/start_frontend.sh\")\nprint(f\"3. Open: http://localhost:8080\")\nprint(f\"4. Paste this URL in the 'Backend Server URL' field\")\nprint(f\"5. Click 'Connect'\")\nprint(f\"\\n‚ö†Ô∏è Keep this URL handy - Cell 6 will keep running!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start Backend Server\n",
    "\n",
    "This will start the FastAPI backend server. The cell will keep running - don't stop it!\n",
    "\n",
    "**What you should see:**\n",
    "- ‚úì Backend directory: ...\n",
    "- ‚úì Model loader created\n",
    "- ‚úì Default model loaded\n",
    "- ‚úì Frame processor created\n",
    "- ‚úÖ Server initialized successfully\n",
    "- INFO: Uvicorn running on http://0.0.0.0:8000\n",
    "\n",
    "**If you see errors:** Check the troubleshooting section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# Allow nested event loops (required for Colab/Jupyter)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure paths\n",
    "backend_dir = '/content/RealTimeSeg/backend'\n",
    "sys.path.insert(0, backend_dir)\n",
    "os.chdir(backend_dir)\n",
    "\n",
    "# Set CUDA device\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "print(f\"‚úì Backend directory: {backend_dir}\")\n",
    "print(f\"‚úì Python path: {sys.path[0]}\")\n",
    "print(f\"‚úì Current directory: {os.getcwd()}\")\n",
    "print(\"\")\n",
    "\n",
    "# Import and initialize\n",
    "try:\n",
    "    print(\"Starting server...\")\n",
    "    from app import app, initialize_server\n",
    "    import uvicorn\n",
    "    \n",
    "    # Initialize server before starting\n",
    "    print(\"Initializing server components...\")\n",
    "    initialize_server()\n",
    "    \n",
    "    # Start uvicorn with Config (compatible with existing event loop)\n",
    "    print(\"Starting uvicorn...\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üöÄ Server is starting...\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\")\n",
    "    \n",
    "    config = uvicorn.Config(\n",
    "        app,\n",
    "        host=\"0.0.0.0\",\n",
    "        port=8000,\n",
    "        log_level=\"info\"\n",
    "    )\n",
    "    server = uvicorn.Server(config)\n",
    "    \n",
    "    # Run in the existing event loop\n",
    "    await server.serve()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to start server: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Keep Session Alive (Optional)\n",
    "\n",
    "Run this in a separate cell to prevent Colab from disconnecting due to inactivity.\n",
    "\n",
    "**Note**: This is optional and should be used responsibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Keep-alive started. Press 'Stop' button to end.\")\n",
    "print(\"This will print a message every 60 seconds.\\n\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"[{current_time}] Session active...\")\n",
    "        time.sleep(60)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nKeep-alive stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Troubleshooting\n\n### Backend Issues (Colab)\n\n**GPU Not Available**\n- Go to `Runtime` ‚Üí `Change runtime type` ‚Üí Select `GPU`\n- Restart runtime and run cells again\n\n**ngrok Connection Failed**\n- Check that your ngrok token is correct\n- Free tier has connection limits (40 connections/min)\n- Try regenerating the tunnel (run Cell 5 again)\n\n**Server Won't Start**\n- Check that all dependencies installed successfully (Cell 3)\n- Make sure Cell 6 shows all ‚úì checkmarks\n- Review error messages in Cell 6 output\n- Try restarting runtime and running all cells again\n\n**\"asyncio.run() cannot be called from a running event loop\"**\n- This is fixed in the updated Cell 6 code\n- Make sure you're using the latest notebook version\n- Cell 6 uses `nest_asyncio` and `await server.serve()` for Colab compatibility\n- If you still see this, re-run Cell 3 to install nest_asyncio\n\n**ImportError: attempted relative import**\n- This means paths aren't configured correctly\n- Make sure Cell 6 runs the proper initialization code\n- The repository should be cloned at `/content/RealTimeSeg`\n\n**TypeError: 'NoneType' object is not callable**\n- Server initialization failed\n- Check Cell 6 output for initialization errors\n- Verify models downloaded successfully in Cell 4\n\n### Frontend Connection Issues (Local)\n\n**\"Connection Failed\" in Browser**\n- Verify Cell 6 is running (should show \"Uvicorn running\")\n- Copy exact ngrok URL from Cell 5 (including https://)\n- Try the test tool: http://localhost:8080/test_connection.html\n- Check if ngrok URL works in a regular browser tab first\n\n**\"JSON parse error\"**\n- Backend is returning HTML instead of JSON\n- This usually means old backend code is running\n- Solution: Re-clone repository in Cell 2 and restart from Cell 3\n\n**WebSocket Connection Failed**\n- HTTP test passed but WebSocket fails?\n- Check CORS settings (should already be configured)\n- Verify you're using wss:// (not ws://) for https URLs\n- The frontend should auto-convert the URL\n\n**Frontend Not Starting Locally**\n- Make sure you're in the RealTimeSeg directory\n- Run: `./scripts/start_frontend.sh` (Linux/Mac) or `scripts\\start_frontend.bat` (Windows)\n- Or manually: `cd frontend && python3 -m http.server 8080`\n- Check that port 8080 isn't already in use\n\n## Performance Tips\n\n1. **Model Selection**:\n   - Fast Mode: 30-40 FPS (MobileNetV3)\n   - Balanced Mode: 20-25 FPS (ResNet50) - **Default**\n   - Accurate Mode: 10-12 FPS (SegFormer-B3)\n   - SOTA Mode: 5-8 FPS (Mask2Former-Swin-Large) - **Best Quality, SOTA Performance**\n\n2. **Monitor GPU**: Run `!nvidia-smi` in a new cell to check GPU usage\n\n3. **First Run is Slower**: Models download (~2-3GB for regular, ~1GB for SOTA), subsequent runs use cache\n\n4. **Session Limits**: \n   - 12-hour maximum session\n   - 90-minute idle disconnect\n   - Run Cell 7 (keep-alive) to prevent idle disconnect\n\n## Testing Connection\n\nUse the diagnostic tool to test connection:\n1. On your local machine, open: http://localhost:8080/test_connection.html\n2. Paste your ngrok URL from Cell 5\n3. Click \"1. Test HTTP\" - should return JSON with server info\n4. Click \"2. Test WebSocket\" - should receive \"connected\" message\n5. If both pass, you can use the main app!\n\n## Architecture Notes\n\nThis setup uses **split architecture**:\n- **Backend (Colab)**: Runs inference with GPU, serves WebSocket API\n- **Frontend (Local)**: Serves UI, captures webcam, renders results\n- **Connection**: WebSocket via ngrok tunnel\n\nBenefits:\n- ‚úÖ Fast local UI (no tunneling latency for static files)\n- ‚úÖ Free Colab GPU for inference\n- ‚úÖ Easy frontend development (just refresh browser)\n- ‚úÖ Better overall performance\n\n## Technical Notes\n\n**Asyncio Compatibility**: Colab notebooks run in an existing asyncio event loop. The server uses `nest_asyncio` and `await server.serve()` instead of `uvicorn.run()` to work within this environment.\n\n**SOTA Model**: The new SOTA mode uses Mask2Former with Swin-Large backbone, achieving 57.7% mIoU on ADE20K (vs 52.4% for SegFormer-B3). It requires more GPU memory (~6.5GB) and runs slower (5-8 FPS) but provides the best segmentation quality available."
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}