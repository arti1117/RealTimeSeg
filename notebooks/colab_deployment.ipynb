{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Real-Time Semantic Segmentation - Google Colab Backend Deployment\n\nThis notebook deploys the **backend server** on Google Colab with GPU acceleration.\n\n## Split Architecture\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  YOUR LOCAL         â”‚         â”‚  GOOGLE COLAB        â”‚\nâ”‚  MACHINE            â”‚         â”‚  (Free GPU)          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                     â”‚         â”‚                      â”‚\nâ”‚  Frontend Server    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤  Backend Server      â”‚\nâ”‚  (Port 8080)        â”‚  ngrok  â”‚  (Port 8000)         â”‚\nâ”‚                     â”‚  WebSocket                     â”‚\nâ”‚  - HTML/CSS/JS      â”‚         â”‚  - FastAPI           â”‚\nâ”‚  - Webcam capture   â”‚         â”‚  - PyTorch Models    â”‚\nâ”‚  - UI controls      â”‚         â”‚  - GPU Inference     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Setup Instructions\n\n1. **Enable GPU**: Go to `Runtime` â†’ `Change runtime type` â†’ Select `GPU`\n2. **Get ngrok token**: Sign up at https://ngrok.com and get your auth token\n3. **Run cells 1-6**: Execute each cell sequentially\n4. **Copy ngrok URL**: From Cell 5 output (you'll paste this in your local frontend)\n5. **Start local frontend**: On your machine, run `./start_frontend.sh`\n6. **Connect**: Open http://localhost:8080 and paste the ngrok URL\n\n## Note\n- Colab sessions timeout after 12 hours or 90 minutes idle\n- GPU allocation is not guaranteed (T4/P100/V100 depending on availability)\n- Frontend runs locally for better performance and easier development"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repository (replace with your repo URL)\n",
    "!git clone https://github.com/yourusername/RealTimeSeg.git\n",
    "%cd RealTimeSeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install PyTorch with CUDA support\n!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n\n# Install other requirements\n!pip install -r backend/requirements.txt\n\n# Install ngrok for tunneling\n!pip install pyngrok\n\n# Install nest_asyncio for Colab event loop compatibility\n!pip install nest_asyncio"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download and Cache Models (Optional)\n",
    "\n",
    "Pre-download models to speed up startup. Models will be cached for the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models.segmentation as models\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "print(\"Downloading models...\")\n",
    "\n",
    "# Download DeepLabV3 models\n",
    "print(\"1/3 Downloading DeepLabV3-MobileNetV3...\")\n",
    "_ = models.deeplabv3_mobilenet_v3_large(pretrained=True)\n",
    "\n",
    "print(\"2/3 Downloading DeepLabV3-ResNet50...\")\n",
    "_ = models.deeplabv3_resnet50(pretrained=True)\n",
    "\n",
    "print(\"3/3 Downloading SegFormer-B3...\")\n",
    "_ = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/segformer-b3-finetuned-ade-512-512\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ All models downloaded and cached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Setup ngrok Tunnel\n\n**Important**: \n1. Replace `YOUR_NGROK_TOKEN` with your actual token from https://ngrok.com\n2. **COPY THE URL** shown below - you'll paste it in your local frontend!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pyngrok import ngrok, conf\nimport os\n\n# Set your ngrok auth token here\nNGROK_TOKEN = \"YOUR_NGROK_TOKEN\"  # Replace with your token from https://ngrok.com\n\n# Configure ngrok\nconf.get_default().auth_token = NGROK_TOKEN\n\n# Create tunnel\npublic_url = ngrok.connect(8000)\nprint(f\"\\n{'='*70}\")\nprint(f\"ğŸŒ PUBLIC URL: {public_url}\")\nprint(f\"{'='*70}\\n\")\nprint(f\"ğŸ“‹ COPY THIS URL - You'll paste it in your local frontend!\")\nprint(f\"\\nNext steps:\")\nprint(f\"1. Copy the URL above\")\nprint(f\"2. On your local machine, run: ./start_frontend.sh\")\nprint(f\"3. Open: http://localhost:8080\")\nprint(f\"4. Paste this URL in the 'Backend Server URL' field\")\nprint(f\"5. Click 'Connect'\")\nprint(f\"\\nâš ï¸ Keep this URL handy - Cell 6 will keep running!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Start Backend Server\n\nThis will start the FastAPI backend server. The cell will keep running - don't stop it!\n\n**What you should see:**\n- âœ“ Backend directory: ...\n- âœ“ Model loader created\n- âœ“ Default model loaded\n- âœ“ Frame processor created\n- âœ… Server initialized successfully\n- INFO: Uvicorn running on http://0.0.0.0:8000\n\n**If you see errors:** Check the troubleshooting section below."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport asyncio\nimport nest_asyncio\n\n# Allow nested event loops (required for Colab/Jupyter)\nnest_asyncio.apply()\n\n# Configure paths\nbackend_dir = '/content/RealTimeSeg/backend'\nsys.path.insert(0, backend_dir)\nos.chdir(backend_dir)\n\n# Set CUDA device\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\nprint(f\"âœ“ Backend directory: {backend_dir}\")\nprint(f\"âœ“ Python path: {sys.path[0]}\")\nprint(f\"âœ“ Current directory: {os.getcwd()}\")\nprint(\"\")\n\n# Import and initialize\ntry:\n    print(\"Starting server...\")\n    from app import app, initialize_server\n    import uvicorn\n    \n    # Initialize server before starting\n    print(\"Initializing server components...\")\n    initialize_server()\n    \n    # Start uvicorn with Config (compatible with existing event loop)\n    print(\"Starting uvicorn...\")\n    print(\"=\" * 70)\n    print(\"ğŸš€ Server is starting...\")\n    print(\"=\" * 70)\n    print(\"\")\n    \n    config = uvicorn.Config(\n        app,\n        host=\"0.0.0.0\",\n        port=8000,\n        log_level=\"info\"\n    )\n    server = uvicorn.Server(config)\n    \n    # Run in the existing event loop\n    await server.serve()\n    \nexcept Exception as e:\n    print(f\"âŒ Failed to start server: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Keep Session Alive (Optional)\n",
    "\n",
    "Run this in a separate cell to prevent Colab from disconnecting due to inactivity.\n",
    "\n",
    "**Note**: This is optional and should be used responsibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Keep-alive started. Press 'Stop' button to end.\")\n",
    "print(\"This will print a message every 60 seconds.\\n\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"[{current_time}] Session active...\")\n",
    "        time.sleep(60)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nKeep-alive stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Troubleshooting\n\n### Backend Issues (Colab)\n\n**GPU Not Available**\n- Go to `Runtime` â†’ `Change runtime type` â†’ Select `GPU`\n- Restart runtime and run cells again\n\n**ngrok Connection Failed**\n- Check that your ngrok token is correct\n- Free tier has connection limits (40 connections/min)\n- Try regenerating the tunnel (run Cell 5 again)\n\n**Server Won't Start**\n- Check that all dependencies installed successfully (Cell 3)\n- Make sure Cell 6 shows all âœ“ checkmarks\n- Review error messages in Cell 6 output\n- Try restarting runtime and running all cells again\n\n**\"asyncio.run() cannot be called from a running event loop\"**\n- This is fixed in the updated Cell 6 code\n- Make sure you're using the latest notebook version\n- Cell 6 uses `nest_asyncio` and `await server.serve()` for Colab compatibility\n- If you still see this, re-run Cell 3 to install nest_asyncio\n\n**ImportError: attempted relative import**\n- This means paths aren't configured correctly\n- Make sure Cell 6 runs the proper initialization code\n- The repository should be cloned at `/content/RealTimeSeg`\n\n**TypeError: 'NoneType' object is not callable**\n- Server initialization failed\n- Check Cell 6 output for initialization errors\n- Verify models downloaded successfully in Cell 4\n\n### Frontend Connection Issues (Local)\n\n**\"Connection Failed\" in Browser**\n- Verify Cell 6 is running (should show \"Uvicorn running\")\n- Copy exact ngrok URL from Cell 5 (including https://)\n- Try the test tool: http://localhost:8080/test_connection.html\n- Check if ngrok URL works in a regular browser tab first\n\n**\"JSON parse error\"**\n- Backend is returning HTML instead of JSON\n- This usually means old backend code is running\n- Solution: Re-clone repository in Cell 2 and restart from Cell 3\n\n**WebSocket Connection Failed**\n- HTTP test passed but WebSocket fails?\n- Check CORS settings (should already be configured)\n- Verify you're using wss:// (not ws://) for https URLs\n- The frontend should auto-convert the URL\n\n**Frontend Not Starting Locally**\n- Make sure you're in the RealTimeSeg directory\n- Run: `./start_frontend.sh` (Linux/Mac) or `start_frontend.bat` (Windows)\n- Or manually: `cd frontend && python3 -m http.server 8080`\n- Check that port 8080 isn't already in use\n\n## Performance Tips\n\n1. **Model Selection**:\n   - Fast Mode: 30-40 FPS (MobileNetV3)\n   - Balanced Mode: 20-25 FPS (ResNet50) - **Default**\n   - Accurate Mode: 10-12 FPS (SegFormer-B3)\n\n2. **Monitor GPU**: Run `!nvidia-smi` in a new cell to check GPU usage\n\n3. **First Run is Slower**: Models download (~2-3GB), subsequent runs use cache\n\n4. **Session Limits**: \n   - 12-hour maximum session\n   - 90-minute idle disconnect\n   - Run Cell 7 (keep-alive) to prevent idle disconnect\n\n## Testing Connection\n\nUse the diagnostic tool to test connection:\n1. On your local machine, open: http://localhost:8080/test_connection.html\n2. Paste your ngrok URL from Cell 5\n3. Click \"1. Test HTTP\" - should return JSON with server info\n4. Click \"2. Test WebSocket\" - should receive \"connected\" message\n5. If both pass, you can use the main app!\n\n## Architecture Notes\n\nThis setup uses **split architecture**:\n- **Backend (Colab)**: Runs inference with GPU, serves WebSocket API\n- **Frontend (Local)**: Serves UI, captures webcam, renders results\n- **Connection**: WebSocket via ngrok tunnel\n\nBenefits:\n- âœ… Fast local UI (no tunneling latency for static files)\n- âœ… Free Colab GPU for inference\n- âœ… Easy frontend development (just refresh browser)\n- âœ… Better overall performance\n\n## Technical Notes\n\n**Asyncio Compatibility**: Colab notebooks run in an existing asyncio event loop. The server uses `nest_asyncio` and `await server.serve()` instead of `uvicorn.run()` to work within this environment."
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}